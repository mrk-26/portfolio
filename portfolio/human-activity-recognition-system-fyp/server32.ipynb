{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q fastapi uvicorn[standard] joblib torch requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [6592]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:60783 - \"POST /predict HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import uvicorn\n",
    "\n",
    "# --- Model architecture ---\n",
    "class HARNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.LeakyReLU(), nn.BatchNorm1d(512), nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256), nn.LeakyReLU(), nn.BatchNorm1d(256), nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128), nn.LeakyReLU(), nn.BatchNorm1d(128), nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64), nn.LeakyReLU(), nn.BatchNorm1d(64), nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Load artifacts ---\n",
    "ARTIFACT_DIR = 'artifacts'\n",
    "SCALER_PATH = os.path.join(ARTIFACT_DIR, 'scaler.joblib')\n",
    "ENCODER_PATH = os.path.join(ARTIFACT_DIR, 'label_encoder.joblib')\n",
    "MODEL_PATH = os.path.join(ARTIFACT_DIR, 'best_model.pth')\n",
    "\n",
    "# Ensure artifacts exist\n",
    "for path in (SCALER_PATH, ENCODER_PATH, MODEL_PATH):\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"Required artifact not found: {path}\")\n",
    "\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "label_encoder = joblib.load(ENCODER_PATH)\n",
    "\n",
    "# Initialize model and load weights\n",
    "def load_model():\n",
    "    input_dim = scaler.scale_.shape[0]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = HARNet(input_dim, num_classes)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "# FastAPI App\n",
    "app = FastAPI(title='HAR 32-Feature Classifier')\n",
    "\n",
    "class Features(BaseModel):\n",
    "    features: list[float]\n",
    "\n",
    "@app.post('/predict')\n",
    "def predict(request: Features):\n",
    "    x = np.array(request.features, dtype=np.float32)  # Ensure float32\n",
    "    expected = scaler.scale_.shape[0]\n",
    "    if x.size != expected:\n",
    "        raise HTTPException(400, detail=f'Expected {expected} features, got {x.size}')\n",
    "    x_scaled = scaler.transform([x])\n",
    "    \n",
    "    # Convert to tensor with explicit float32 dtype\n",
    "    input_tensor = torch.from_numpy(x_scaled).float()  # <-- Add .float() here\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "        # item() gives a Python scalar, but letâ€™s be explicit:\n",
    "        idx = torch.argmax(logits, dim=1).item()  # this is now a Python int\n",
    "\n",
    "    activity = label_encoder.inverse_transform([idx])[0]\n",
    "    # activity may be a numpy.str_, so cast it:\n",
    "    activity = str(activity)\n",
    "\n",
    "    return {\n",
    "        'activity': activity,    # guaranteed to be a plain str\n",
    "        'index': int(idx)        # guaranteed to be a plain int\n",
    "    }\n",
    "if __name__ == '__main__':\n",
    "    import asyncio\n",
    "    \n",
    "    async def run_server():\n",
    "        config = uvicorn.Config(\n",
    "            app,\n",
    "            host='0.0.0.0',\n",
    "            port=int(os.getenv('PORT', 8000)),\n",
    "            reload=False\n",
    "        )\n",
    "        server = uvicorn.Server(config)\n",
    "        await server.serve()\n",
    "\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # For environments with existing event loop (e.g., Jupyter)\n",
    "            loop.create_task(run_server())\n",
    "        else:\n",
    "            # Normal execution\n",
    "            asyncio.run(run_server())\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}. Try running with a higher port or elevated permissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
