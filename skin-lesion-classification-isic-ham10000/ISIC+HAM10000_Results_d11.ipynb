{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"nodoubttome/skin-cancer9-classesisic\")"
      ],
      "metadata": {
        "id": "Z1rhGp_edhgZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcFwE5w8duS6",
        "outputId": "3834385b-f261-4994-a655-02c201048297"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/skin-cancer9-classesisic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LQbJCnhWWSF",
        "outputId": "9df127c8-5c9b-4bcf-91e9-39224ea3b3f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images (filtered): 2080\n",
            "\n",
            "--- Train Split ---\n",
            "\n",
            "Model: GLCM+ViT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       akiec     0.4542    1.0000    0.6247       114\n",
            "         bcc     0.9210    0.9920    0.9552       376\n",
            "         bkl     0.9311    0.9069    0.9189       462\n",
            "          df     0.9588    0.9789    0.9688        95\n",
            "         mel     0.7216    0.1598    0.2617       438\n",
            "          nv     0.4510    0.6835    0.5434       357\n",
            "        vasc     0.9929    1.0000    0.9964       139\n",
            "\n",
            "    accuracy                         0.7330      1981\n",
            "   macro avg     0.7758    0.8173    0.7527      1981\n",
            "weighted avg     0.7746    0.7330    0.7037      1981\n",
            "\n",
            "Overall accuracy: 0.7329631499242807\n",
            "\n",
            "Model: ViT-Only\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       akiec     0.4728    0.9912    0.6402       114\n",
            "         bcc     0.9093    0.9867    0.9464       376\n",
            "         bkl     0.8979    0.9134    0.9056       462\n",
            "          df     0.9314    1.0000    0.9645        95\n",
            "         mel     0.6947    0.1507    0.2477       438\n",
            "          nv     0.4590    0.6751    0.5465       357\n",
            "        vasc     0.9789    1.0000    0.9893       139\n",
            "\n",
            "    accuracy                         0.7304      1981\n",
            "   macro avg     0.7634    0.8167    0.7486      1981\n",
            "weighted avg     0.7589    0.7304    0.6966      1981\n",
            "\n",
            "Overall accuracy: 0.7304391721352852\n",
            "\n",
            "--- Test Split ---\n",
            "\n",
            "Model: GLCM+ViT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       akiec     0.9286    0.8125    0.8667        16\n",
            "         bcc     1.0000    1.0000    1.0000        16\n",
            "         bkl     0.9375    0.9375    0.9375        16\n",
            "          df     1.0000    0.5625    0.7200        16\n",
            "         mel     0.7500    0.3750    0.5000        16\n",
            "          nv     0.4545    0.9375    0.6122        16\n",
            "        vasc     1.0000    1.0000    1.0000         3\n",
            "\n",
            "    accuracy                         0.7778        99\n",
            "   macro avg     0.8672    0.8036    0.8052        99\n",
            "weighted avg     0.8498    0.7778    0.7796        99\n",
            "\n",
            "Overall accuracy: 0.7777777777777778\n",
            "\n",
            "Model: ViT-Only\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       akiec     1.0000    0.8750    0.9333        16\n",
            "         bcc     0.9412    1.0000    0.9697        16\n",
            "         bkl     0.8667    0.8125    0.8387        16\n",
            "          df     1.0000    0.6250    0.7692        16\n",
            "         mel     0.8000    0.2500    0.3810        16\n",
            "          nv     0.4118    0.8750    0.5600        16\n",
            "        vasc     0.7500    1.0000    0.8571         3\n",
            "\n",
            "    accuracy                         0.7475        99\n",
            "   macro avg     0.8242    0.7768    0.7584        99\n",
            "weighted avg     0.8340    0.7475    0.7455        99\n",
            "\n",
            "Overall accuracy: 0.7474747474747475\n"
          ]
        }
      ],
      "source": [
        "# 1) Install & Imports\n",
        "#!pip install torch torchvision transformers scikit-image scikit-learn opencv-python kagglehub\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import SwinForImageClassification\n",
        "from torchvision import transforms\n",
        "# from skimage.feature import graycomatrix, graycoprops\n",
        "from skimage.feature import graycomatrix as greycomatrix, graycoprops as greycoprops\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "# import kagglehub\n",
        "from torch.serialization import add_safe_globals\n",
        "\n",
        "# 2) Download & unzip (if you haven’t already)\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# path = kagglehub.dataset_download(\"nodoubttome/skin-cancer9-classesisic\")\n",
        "# !unzip -q \"{path}\" -d /content/isic\n",
        "\n",
        "class HybridGLCMSwin(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_name=\"microsoft/swin-large-patch4-window7-224-in22k\",\n",
        "                 num_glcm_feats=6,\n",
        "                 glcm_emb_dim=128,\n",
        "                 num_classes=7):\n",
        "        super().__init__()\n",
        "        # Swin backbone\n",
        "        self.swin = SwinForImageClassification.from_pretrained(model_name)\n",
        "        embed_dim = self.swin.config.hidden_size\n",
        "\n",
        "        # MLP for GLCM\n",
        "        self.glcm_mlp = nn.Sequential(\n",
        "            nn.Linear(num_glcm_feats, glcm_emb_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "        )\n",
        "\n",
        "        # final classifier on concatenated features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim + glcm_emb_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_img, x_glcm):\n",
        "        # Swin forward\n",
        "        swin_out = self.swin(pixel_values=x_img, output_hidden_states=True)\n",
        "        swin_feat = swin_out.hidden_states[-1][:,0]               # [B, embed_dim]\n",
        "\n",
        "        # GLCM embedding\n",
        "        glcm_feat = self.glcm_mlp(x_glcm)                         # [B, glcm_emb_dim]\n",
        "\n",
        "        # fuse\n",
        "        joint = torch.cat([swin_feat, glcm_feat], dim=1)          # [B, embed+glcm]\n",
        "\n",
        "        return self.classifier(joint)\n",
        "\n",
        "class ModifiedPretrainedSwinV2(nn.Module):\n",
        "    def __init__(self, model_name=\"microsoft/swin-large-patch4-window7-224-in22k\", num_classes=7):\n",
        "        super(ModifiedPretrainedSwinV2, self).__init__()\n",
        "        print(\"Loading Swin Transformer model...\")\n",
        "        self.swin = SwinForImageClassification.from_pretrained(model_name)\n",
        "        self.embed_dim = self.swin.config.hidden_size\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        swin_outputs = self.swin(pixel_values=x, output_hidden_states=True)\n",
        "        pooled_output = swin_outputs.hidden_states[-1][:, 0]\n",
        "        outputs = self.classifier(pooled_output)\n",
        "        return outputs\n",
        "\n",
        "# 3) Define the mapping from folder names → HAM10000 labels\n",
        "dir2label = {\n",
        "    \"actinic keratosis\":          \"akiec\",\n",
        "    \"basal cell carcinoma\":       \"bcc\",\n",
        "    \"pigmented benign keratosis\": \"bkl\",\n",
        "    \"dermatofibroma\":             \"df\",\n",
        "    \"melanoma\":                   \"mel\",\n",
        "    \"nevus\":                      \"nv\",\n",
        "    \"vascular lesion\":            \"vasc\"\n",
        "}\n",
        "\n",
        "ham_classes = list(dir2label.values())\n",
        "\n",
        "# 4) Collect (image_path, label) from Train & Test\n",
        "def make_dataset(root_dir):\n",
        "    data = []\n",
        "    for split in (\"Train\", \"Test\"):\n",
        "        split_dir = os.path.join(root_dir, split)\n",
        "        for folder in os.listdir(split_dir):\n",
        "            lbl = dir2label.get(folder.lower())\n",
        "            if lbl is None:\n",
        "                continue  # skip SCC, SK, etc.\n",
        "            folder_path = os.path.join(split_dir, folder)\n",
        "            for fname in os.listdir(folder_path):\n",
        "                if fname.lower().endswith((\".jpg\",\".png\")):\n",
        "                    data.append((os.path.join(folder_path, fname), lbl, split))\n",
        "    return data\n",
        "\n",
        "dataset = make_dataset(\"/kaggle/input/skin-cancer9-classesisic/Skin cancer ISIC The International Skin Imaging Collaboration\")  # adjust if your unzip path differs\n",
        "print(f\"Total images (filtered): {len(dataset)}\")\n",
        "\n",
        "# 5) Preprocess & GLCM helpers\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "def preprocess(img_path):\n",
        "    img = cv2.imread(img_path)[:,:,::-1]\n",
        "    return transform(img).unsqueeze(0).to(device), img\n",
        "\n",
        "def compute_glcm(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    q = (gray / 32).astype(np.uint8)\n",
        "    glcm = greycomatrix(q, distances=[1,2,4],\n",
        "                        angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
        "                        levels=8, symmetric=True, normed=True)\n",
        "    feats = [greycoprops(glcm, p).mean()\n",
        "             for p in ['contrast','dissimilarity','homogeneity','energy','correlation','ASM']]\n",
        "    return torch.tensor(feats, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "# 1) Allowlist your model classes for unpickling\n",
        "add_safe_globals([HybridGLCMSwin, ModifiedPretrainedSwinV2])\n",
        "\n",
        "def load_model(path, cls):\n",
        "    \"\"\"\n",
        "    Loads a checkpoint that may contain the full model object.\n",
        "    We allow unpickling of our custom class, and set weights_only=False\n",
        "    so that torch.load will actually reconstruct the object.\n",
        "    \"\"\"\n",
        "    # map_location ensures it works on GPU or CPU\n",
        "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
        "\n",
        "    # If checkpoint is a full nn.Module instance\n",
        "    if isinstance(ckpt, nn.Module):\n",
        "        model = ckpt.to(device)\n",
        "    # If checkpoint is a dict/state_dict\n",
        "    elif isinstance(ckpt, dict):\n",
        "        model = cls().to(device)\n",
        "        # some people wrap with ['model_state_dict']\n",
        "        if \"state_dict\" in ckpt:\n",
        "            state_dict = ckpt[\"state_dict\"]\n",
        "        else:\n",
        "            state_dict = ckpt\n",
        "        model.load_state_dict(state_dict)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported checkpoint format: {type(ckpt)}\")\n",
        "\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "glcm_model = load_model(\"/content/drive/MyDrive/swin+GLCM_complete_trained_model_ALL_FOLDS.pth\", HybridGLCMSwin)\n",
        "vit_model  = load_model(\"/content/drive/MyDrive/swin_complete_trained_model_ALL_FOLDS.pth\",   ModifiedPretrainedSwinV2)\n",
        "\n",
        "# 7) Run inference & gather preds per split\n",
        "results = {\n",
        "    \"Train\": {\"true\": [], \"glcm\": [], \"vit\": []},\n",
        "    \"Test\":  {\"true\": [], \"glcm\": [], \"vit\": []}\n",
        "}\n",
        "for img_path, label, split in dataset:\n",
        "    inp, raw = preprocess(img_path)\n",
        "    glcm_feats = compute_glcm(raw)\n",
        "    with torch.no_grad():\n",
        "        out1 = glcm_model(inp, glcm_feats)\n",
        "        out2 = vit_model(inp)\n",
        "    p1 = F.softmax(out1, dim=1).argmax(dim=1).item()\n",
        "    p2 = F.softmax(out2, dim=1).argmax(dim=1).item()\n",
        "    # reverse-map index→label\n",
        "    idx2lbl = {i:ham_classes[i] for i in range(len(ham_classes))}\n",
        "    results[split][\"true\"].append(label)\n",
        "    results[split][\"glcm\"].append(idx2lbl[p1])\n",
        "    results[split][\"vit\"].append(idx2lbl[p2])\n",
        "\n",
        "# 8) Compute & print metrics for each split & model\n",
        "for split in (\"Train\",\"Test\"):\n",
        "    print(f\"\\n--- {split} Split ---\")\n",
        "    y_true = results[split][\"true\"]\n",
        "    for name, y_pred in ((\"GLCM+ViT\", results[split][\"glcm\"]),\n",
        "                         (\"ViT-Only\", results[split][\"vit\"])):\n",
        "        print(f\"\\nModel: {name}\")\n",
        "        print(classification_report(\n",
        "            y_true, y_pred, labels=ham_classes, digits=4))\n",
        "        print(\"Overall accuracy:\", accuracy_score(y_true, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Install & import kagglehub\n",
        "!pip install kagglehub\n",
        "\n",
        "import os\n",
        "import kagglehub\n",
        "import zipfile\n",
        "\n",
        "# 1) Download the HAM10000 dataset\n",
        "path = kagglehub.dataset_download(\"kmader/skin-cancer-mnist-ham10000\")\n",
        "print(\"kagglehub returned:\", path)\n",
        "\n",
        "# 2) Determine if 'path' is a directory or a zip file\n",
        "if os.path.isdir(path):\n",
        "    # Already unpacked\n",
        "    WORK_DIR = path\n",
        "    print(f\"Using directory: {WORK_DIR}\")\n",
        "else:\n",
        "    # It's a zip file → unzip it\n",
        "    WORK_DIR = \"/content/HAM10000\"\n",
        "    os.makedirs(WORK_DIR, exist_ok=True)\n",
        "    with zipfile.ZipFile(path, 'r') as z:\n",
        "        z.extractall(WORK_DIR)\n",
        "    print(f\"Unzipped to: {WORK_DIR}\")\n",
        "\n",
        "# 3) Set metadata and image directories\n",
        "METADATA_CSV = os.path.join(WORK_DIR, \"HAM10000_metadata.csv\")\n",
        "IMAGE_DIRS = [\n",
        "    os.path.join(WORK_DIR, \"HAM10000_images_part_1\"),\n",
        "    os.path.join(WORK_DIR, \"HAM10000_images_part_2\"),\n",
        "]\n",
        "\n",
        "# 4) Quick sanity check\n",
        "print(\"Metadata exists:\", os.path.exists(METADATA_CSV))\n",
        "print(\"Image dir 1 exists:\", os.path.isdir(IMAGE_DIRS[0]))\n",
        "print(\"Image dir 2 exists:\", os.path.isdir(IMAGE_DIRS[1]))\n",
        "\n",
        "# 5) Load metadata & list images (example)\n",
        "import pandas as pd\n",
        "meta = pd.read_csv(METADATA_CSV)\n",
        "print(\"Metadata rows:\", len(meta))\n",
        "\n",
        "# Gather a small sample to verify paths\n",
        "sample = []\n",
        "for bd in IMAGE_DIRS:\n",
        "    for fname in os.listdir(bd)[:5]:\n",
        "        sample.append(os.path.join(bd, fname))\n",
        "print(\"Sample image paths:\", sample)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnEiTeKhYlPQ",
        "outputId": "10e4b215-75c8-4be7-f63b-715e44f081b8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.4.26)\n",
            "kagglehub returned: /kaggle/input/skin-cancer-mnist-ham10000\n",
            "Using directory: /kaggle/input/skin-cancer-mnist-ham10000\n",
            "Metadata exists: True\n",
            "Image dir 1 exists: True\n",
            "Image dir 2 exists: True\n",
            "Metadata rows: 10015\n",
            "Sample image paths: ['/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1/ISIC_0028933.jpg', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1/ISIC_0028394.jpg', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1/ISIC_0027799.jpg', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1/ISIC_0028100.jpg', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1/ISIC_0027960.jpg', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2/ISIC_0030912.jpg', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2/ISIC_0030585.jpg', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2/ISIC_0033697.jpg', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2/ISIC_0030062.jpg', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2/ISIC_0031213.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Install & imports\n",
        "# (uncomment if running in Colab)\n",
        "# !pip install torch torchvision transformers scikit-image scikit-learn opencv-python pandas\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from transformers import SwinForImageClassification\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from torch.serialization import add_safe_globals\n",
        "\n",
        "# 2) Model definitions\n",
        "class HybridGLCMSwin(nn.Module):\n",
        "    def __init__(self, swin_model_name=\"microsoft/swin-large-patch4-window7-224-in22k\",\n",
        "                 num_glcm_feats=6, glcm_emb_dim=128, num_classes=7):\n",
        "        super().__init__()\n",
        "        self.swin = SwinForImageClassification.from_pretrained(swin_model_name)\n",
        "        embed_dim = self.swin.config.hidden_size\n",
        "        self.glcm_mlp = nn.Sequential(\n",
        "            nn.Linear(num_glcm_feats, glcm_emb_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim + glcm_emb_dim, 512),\n",
        "            nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    def forward(self, x_img, x_glcm):\n",
        "        swin_out = self.swin(pixel_values=x_img, output_hidden_states=True)\n",
        "        swin_feat = swin_out.hidden_states[-1][:, 0]\n",
        "        glcm_feat = self.glcm_mlp(x_glcm)\n",
        "        joint = torch.cat([swin_feat, glcm_feat], dim=1)\n",
        "        return self.classifier(joint)\n",
        "\n",
        "class ModifiedPretrainedSwinV2(nn.Module):\n",
        "    def __init__(self, model_name=\"microsoft/swin-large-patch4-window7-224-in22k\",\n",
        "                 num_classes=7):\n",
        "        super().__init__()\n",
        "        self.swin = SwinForImageClassification.from_pretrained(model_name)\n",
        "        embed_dim = self.swin.config.hidden_size\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 512),\n",
        "            nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out = self.swin(pixel_values=x, output_hidden_states=True)\n",
        "        pooled = out.hidden_states[-1][:, 0]\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "# 3) Register safe globals for unpickling\n",
        "add_safe_globals([HybridGLCMSwin, ModifiedPretrainedSwinV2])\n",
        "\n",
        "# 4) Utility: load a checkpoint (full module or state-dict)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def load_model(path, cls):\n",
        "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
        "    if isinstance(ckpt, nn.Module):\n",
        "        model = ckpt.to(device)\n",
        "    elif isinstance(ckpt, dict):\n",
        "        model = cls().to(device)\n",
        "        sd = ckpt.get(\"state_dict\", ckpt)\n",
        "        model.load_state_dict(sd)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported checkpoint format: {type(ckpt)}\")\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# 5) Preprocessing & GLCM helpers\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "def preprocess(img_path):\n",
        "    img = cv2.imread(img_path)[:,:,::-1]\n",
        "    return transform(img).unsqueeze(0).to(device), img\n",
        "\n",
        "def compute_glcm(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    q = (gray / 32).astype(np.uint8)\n",
        "    glcm = graycomatrix(q,\n",
        "                        distances=[1,2,4],\n",
        "                        angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
        "                        levels=8, symmetric=True, normed=True)\n",
        "    feats = [greycoprops(glcm, p).mean()\n",
        "             for p in ['contrast','dissimilarity','homogeneity','energy','correlation','ASM']]\n",
        "    return torch.tensor(feats, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "# 6) Load your models (update these to wherever you’ve placed them)\n",
        "glcm_model = load_model(\"/content/drive/MyDrive/swin+GLCM_complete_trained_model_ALL_FOLDS.pth\",\n",
        "                        HybridGLCMSwin)\n",
        "vit_model  = load_model(\"/content/drive/MyDrive/swin_complete_trained_model_ALL_FOLDS.pth\",\n",
        "                        ModifiedPretrainedSwinV2)\n",
        "\n",
        "# 7) Paths for HAM10000 under Kaggle input\n",
        "BASE = \"/kaggle/input/skin-cancer-mnist-ham10000\"\n",
        "METADATA_CSV = os.path.join(BASE, \"HAM10000_metadata.csv\")\n",
        "IMAGE_DIRS = [\n",
        "    os.path.join(BASE, \"HAM10000_images_part_1\"),\n",
        "    os.path.join(BASE, \"HAM10000_images_part_2\"),\n",
        "]\n",
        "\n",
        "# 8) Load metadata & build data list\n",
        "meta = pd.read_csv(METADATA_CSV)\n",
        "classes = [\"akiec\",\"bcc\",\"bkl\",\"df\",\"mel\",\"nv\",\"vasc\"]\n",
        "cls2idx = {c:i for i,c in enumerate(classes)}\n",
        "idx2cls = {i:c for i,c in enumerate(classes)}\n",
        "\n",
        "data = []\n",
        "for bd in IMAGE_DIRS:\n",
        "    for fname in os.listdir(bd):\n",
        "        if not fname.lower().endswith(\".jpg\"):\n",
        "            continue\n",
        "        image_id = os.path.splitext(fname)[0]\n",
        "        row = meta[meta[\"image_id\"] == image_id]\n",
        "        if row.empty:\n",
        "            continue\n",
        "        label = row[\"dx\"].values[0]\n",
        "        data.append((os.path.join(bd, fname), label))\n",
        "\n",
        "print(f\"Total HAM10000 images: {len(data)}\")\n",
        "\n",
        "# 9) Run inference & collect preds\n",
        "y_true, y_pred_glcm, y_pred_vit = [], [], []\n",
        "\n",
        "for img_path, label in data:\n",
        "    inp, raw = preprocess(img_path)\n",
        "    glcm_feats = compute_glcm(raw)\n",
        "    with torch.no_grad():\n",
        "        out1 = glcm_model(inp, glcm_feats)\n",
        "        out2 = vit_model(inp)\n",
        "    p1 = F.softmax(out1, dim=1).argmax(dim=1).item()\n",
        "    p2 = F.softmax(out2, dim=1).argmax(dim=1).item()\n",
        "\n",
        "    y_true.append(label)\n",
        "    y_pred_glcm.append(idx2cls[p1])\n",
        "    y_pred_vit.append(idx2cls[p2])\n",
        "\n",
        "# 10) Print per-class metrics & overall accuracy\n",
        "print(\"=== GLCM + Swin Model ===\")\n",
        "print(classification_report(y_true, y_pred_glcm, labels=classes, digits=4))\n",
        "print(\"Overall accuracy:\", accuracy_score(y_true, y_pred_glcm))\n",
        "\n",
        "print(\"\\n=== Swin-Only Model ===\")\n",
        "print(classification_report(y_true, y_pred_vit, labels=classes, digits=4))\n",
        "print(\"Overall accuracy:\", accuracy_score(y_true, y_pred_vit))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHcqDzQbhnya",
        "outputId": "ba1d0acd-a06b-46c8-8ab5-f479a07dba26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total HAM10000 images: 10015\n",
            "=== GLCM + Swin Model ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       akiec     0.9076    0.9908    0.9474       327\n",
            "         bcc     0.9827    0.9942    0.9884       514\n",
            "         bkl     0.9951    0.9217    0.9570      1099\n",
            "          df     0.9741    0.9826    0.9784       115\n",
            "         mel     0.9971    0.9200    0.9570      1113\n",
            "          nv     0.9800    0.9990    0.9894      6705\n",
            "        vasc     1.0000    1.0000    1.0000       142\n",
            "\n",
            "    accuracy                         0.9810     10015\n",
            "   macro avg     0.9766    0.9726    0.9739     10015\n",
            "weighted avg     0.9815    0.9810    0.9808     10015\n",
            "\n",
            "Overall accuracy: 0.981028457314029\n",
            "\n",
            "=== Swin-Only Model ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       akiec     0.9637    0.9755    0.9696       327\n",
            "         bcc     0.9751    0.9903    0.9826       514\n",
            "         bkl     0.9971    0.9336    0.9643      1099\n",
            "          df     0.9744    0.9913    0.9828       115\n",
            "         mel     1.0000    0.9084    0.9520      1113\n",
            "          nv     0.9765    0.9994    0.9878      6705\n",
            "        vasc     0.9930    1.0000    0.9965       142\n",
            "\n",
            "    accuracy                         0.9807     10015\n",
            "   macro avg     0.9828    0.9712    0.9765     10015\n",
            "weighted avg     0.9811    0.9807    0.9805     10015\n",
            "\n",
            "Overall accuracy: 0.98072890664004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EOnE4EleiMZP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}