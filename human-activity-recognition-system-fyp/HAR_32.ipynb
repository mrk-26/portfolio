{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss 1.3937, acc 0.4882 | Val loss 1.5314, acc 0.5452\n",
      "Epoch 2: Train loss 0.8464, acc 0.7269 | Val loss 1.0891, acc 0.7845\n",
      "Epoch 3: Train loss 0.6630, acc 0.7800 | Val loss 0.7267, acc 0.8083\n",
      "Epoch 4: Train loss 0.5526, acc 0.8158 | Val loss 0.5191, acc 0.8470\n",
      "Epoch 5: Train loss 0.4848, acc 0.8332 | Val loss 0.4155, acc 0.8647\n",
      "Epoch 6: Train loss 0.4408, acc 0.8507 | Val loss 0.3612, acc 0.8783\n",
      "Epoch 7: Train loss 0.4053, acc 0.8629 | Val loss 0.3260, acc 0.8838\n",
      "Epoch 8: Train loss 0.3753, acc 0.8689 | Val loss 0.3044, acc 0.8851\n",
      "Epoch 9: Train loss 0.3476, acc 0.8791 | Val loss 0.2863, acc 0.8899\n",
      "Epoch 10: Train loss 0.3377, acc 0.8822 | Val loss 0.2737, acc 0.8967\n",
      "Epoch 11: Train loss 0.3261, acc 0.8796 | Val loss 0.2634, acc 0.9014\n",
      "Epoch 12: Train loss 0.3181, acc 0.8825 | Val loss 0.2604, acc 0.8926\n",
      "Epoch 13: Train loss 0.2982, acc 0.8925 | Val loss 0.2485, acc 0.9062\n",
      "Epoch 14: Train loss 0.2870, acc 0.8934 | Val loss 0.2430, acc 0.9021\n",
      "Epoch 15: Train loss 0.2829, acc 0.8937 | Val loss 0.2388, acc 0.9069\n",
      "Epoch 16: Train loss 0.2706, acc 0.9036 | Val loss 0.2294, acc 0.9096\n",
      "Epoch 17: Train loss 0.2716, acc 0.8997 | Val loss 0.2264, acc 0.9062\n",
      "Epoch 18: Train loss 0.2616, acc 0.9051 | Val loss 0.2173, acc 0.9177\n",
      "Epoch 19: Train loss 0.2549, acc 0.9058 | Val loss 0.2150, acc 0.9150\n",
      "Epoch 20: Train loss 0.2510, acc 0.9075 | Val loss 0.2105, acc 0.9177\n",
      "Epoch 21: Train loss 0.2416, acc 0.9167 | Val loss 0.2051, acc 0.9177\n",
      "Epoch 22: Train loss 0.2359, acc 0.9131 | Val loss 0.2023, acc 0.9198\n",
      "Epoch 23: Train loss 0.2336, acc 0.9157 | Val loss 0.1994, acc 0.9211\n",
      "Epoch 24: Train loss 0.2287, acc 0.9126 | Val loss 0.1963, acc 0.9198\n",
      "Epoch 25: Train loss 0.2281, acc 0.9170 | Val loss 0.1921, acc 0.9259\n",
      "Epoch 26: Train loss 0.2218, acc 0.9165 | Val loss 0.1893, acc 0.9252\n",
      "Epoch 27: Train loss 0.2150, acc 0.9220 | Val loss 0.1931, acc 0.9218\n",
      "Epoch 28: Train loss 0.2096, acc 0.9225 | Val loss 0.1868, acc 0.9259\n",
      "Epoch 29: Train loss 0.2140, acc 0.9182 | Val loss 0.1826, acc 0.9341\n",
      "Epoch 30: Train loss 0.2048, acc 0.9235 | Val loss 0.1778, acc 0.9320\n",
      "Epoch 31: Train loss 0.2078, acc 0.9242 | Val loss 0.1749, acc 0.9320\n",
      "Epoch 32: Train loss 0.1967, acc 0.9274 | Val loss 0.1715, acc 0.9341\n",
      "Epoch 33: Train loss 0.1974, acc 0.9274 | Val loss 0.1686, acc 0.9368\n",
      "Epoch 34: Train loss 0.1932, acc 0.9281 | Val loss 0.1687, acc 0.9327\n",
      "Epoch 35: Train loss 0.1947, acc 0.9272 | Val loss 0.1691, acc 0.9307\n",
      "Epoch 36: Train loss 0.1840, acc 0.9296 | Val loss 0.1671, acc 0.9375\n",
      "Epoch 37: Train loss 0.1809, acc 0.9342 | Val loss 0.1644, acc 0.9347\n",
      "Epoch 38: Train loss 0.1843, acc 0.9293 | Val loss 0.1626, acc 0.9375\n",
      "Epoch 39: Train loss 0.1821, acc 0.9316 | Val loss 0.1645, acc 0.9347\n",
      "Epoch 40: Train loss 0.1740, acc 0.9356 | Val loss 0.1579, acc 0.9388\n",
      "Epoch 41: Train loss 0.1763, acc 0.9333 | Val loss 0.1560, acc 0.9409\n",
      "Epoch 42: Train loss 0.1750, acc 0.9328 | Val loss 0.1539, acc 0.9409\n",
      "Epoch 43: Train loss 0.1713, acc 0.9369 | Val loss 0.1524, acc 0.9388\n",
      "Epoch 44: Train loss 0.1701, acc 0.9373 | Val loss 0.1504, acc 0.9422\n",
      "Epoch 45: Train loss 0.1633, acc 0.9373 | Val loss 0.1471, acc 0.9415\n",
      "Epoch 46: Train loss 0.1671, acc 0.9373 | Val loss 0.1467, acc 0.9388\n",
      "Epoch 47: Train loss 0.1652, acc 0.9367 | Val loss 0.1470, acc 0.9415\n",
      "Epoch 48: Train loss 0.1585, acc 0.9413 | Val loss 0.1409, acc 0.9436\n",
      "Epoch 49: Train loss 0.1562, acc 0.9432 | Val loss 0.1444, acc 0.9443\n",
      "Epoch 50: Train loss 0.1637, acc 0.9388 | Val loss 0.1408, acc 0.9443\n",
      "Epoch 51: Train loss 0.1565, acc 0.9403 | Val loss 0.1397, acc 0.9443\n",
      "Epoch 52: Train loss 0.1539, acc 0.9393 | Val loss 0.1380, acc 0.9436\n",
      "Epoch 53: Train loss 0.1413, acc 0.9468 | Val loss 0.1399, acc 0.9456\n",
      "Epoch 54: Train loss 0.1485, acc 0.9458 | Val loss 0.1407, acc 0.9443\n",
      "Epoch 55: Train loss 0.1461, acc 0.9449 | Val loss 0.1375, acc 0.9429\n",
      "Epoch 56: Train loss 0.1491, acc 0.9430 | Val loss 0.1384, acc 0.9429\n",
      "Epoch 57: Train loss 0.1436, acc 0.9432 | Val loss 0.1405, acc 0.9422\n",
      "Epoch 58: Train loss 0.1556, acc 0.9398 | Val loss 0.1383, acc 0.9449\n",
      "Epoch 59: Train loss 0.1547, acc 0.9425 | Val loss 0.1406, acc 0.9395\n",
      "Epoch 60: Train loss 0.1441, acc 0.9447 | Val loss 0.1388, acc 0.9443\n",
      "Epoch 61: Train loss 0.1451, acc 0.9471 | Val loss 0.1375, acc 0.9449\n",
      "Epoch 62: Train loss 0.1423, acc 0.9454 | Val loss 0.1283, acc 0.9490\n",
      "Epoch 63: Train loss 0.1424, acc 0.9458 | Val loss 0.1281, acc 0.9490\n",
      "Epoch 64: Train loss 0.1446, acc 0.9463 | Val loss 0.1299, acc 0.9497\n",
      "Epoch 65: Train loss 0.1361, acc 0.9490 | Val loss 0.1312, acc 0.9470\n",
      "Epoch 66: Train loss 0.1414, acc 0.9485 | Val loss 0.1311, acc 0.9483\n",
      "Epoch 67: Train loss 0.1423, acc 0.9449 | Val loss 0.1248, acc 0.9531\n",
      "Epoch 68: Train loss 0.1311, acc 0.9512 | Val loss 0.1257, acc 0.9504\n",
      "Epoch 69: Train loss 0.1401, acc 0.9493 | Val loss 0.1325, acc 0.9490\n",
      "Epoch 70: Train loss 0.1367, acc 0.9451 | Val loss 0.1322, acc 0.9477\n",
      "Epoch 71: Train loss 0.1309, acc 0.9502 | Val loss 0.1308, acc 0.9463\n",
      "Epoch 72: Train loss 0.1380, acc 0.9478 | Val loss 0.1254, acc 0.9504\n",
      "Epoch 73: Train loss 0.1330, acc 0.9520 | Val loss 0.1243, acc 0.9531\n",
      "Epoch 74: Train loss 0.1307, acc 0.9529 | Val loss 0.1234, acc 0.9497\n",
      "Epoch 75: Train loss 0.1266, acc 0.9529 | Val loss 0.1207, acc 0.9511\n",
      "Epoch 76: Train loss 0.1340, acc 0.9505 | Val loss 0.1221, acc 0.9497\n",
      "Epoch 77: Train loss 0.1260, acc 0.9520 | Val loss 0.1235, acc 0.9504\n",
      "Epoch 78: Train loss 0.1240, acc 0.9520 | Val loss 0.1216, acc 0.9511\n",
      "Epoch 79: Train loss 0.1299, acc 0.9509 | Val loss 0.1141, acc 0.9565\n",
      "Epoch 80: Train loss 0.1221, acc 0.9556 | Val loss 0.1156, acc 0.9565\n",
      "Epoch 81: Train loss 0.1270, acc 0.9534 | Val loss 0.1228, acc 0.9504\n",
      "Epoch 82: Train loss 0.1253, acc 0.9514 | Val loss 0.1297, acc 0.9490\n",
      "Epoch 83: Train loss 0.1165, acc 0.9558 | Val loss 0.1222, acc 0.9517\n",
      "Epoch 84: Train loss 0.1261, acc 0.9509 | Val loss 0.1145, acc 0.9545\n",
      "Epoch 85: Train loss 0.1264, acc 0.9522 | Val loss 0.1188, acc 0.9545\n",
      "Epoch 86: Train loss 0.1174, acc 0.9548 | Val loss 0.1179, acc 0.9538\n",
      "Epoch 87: Train loss 0.1180, acc 0.9551 | Val loss 0.1171, acc 0.9517\n",
      "Epoch 88: Train loss 0.1155, acc 0.9553 | Val loss 0.1164, acc 0.9558\n",
      "Epoch 89: Train loss 0.1166, acc 0.9543 | Val loss 0.1135, acc 0.9545\n",
      "Epoch 90: Train loss 0.1186, acc 0.9548 | Val loss 0.1116, acc 0.9558\n",
      "Epoch 91: Train loss 0.1103, acc 0.9587 | Val loss 0.1108, acc 0.9592\n",
      "Epoch 92: Train loss 0.1115, acc 0.9577 | Val loss 0.1211, acc 0.9524\n",
      "Epoch 93: Train loss 0.1210, acc 0.9522 | Val loss 0.1227, acc 0.9511\n",
      "Epoch 94: Train loss 0.1169, acc 0.9560 | Val loss 0.1132, acc 0.9565\n",
      "Epoch 95: Train loss 0.1155, acc 0.9546 | Val loss 0.1118, acc 0.9565\n",
      "Epoch 96: Train loss 0.1155, acc 0.9558 | Val loss 0.1190, acc 0.9497\n",
      "Epoch 97: Train loss 0.1198, acc 0.9515 | Val loss 0.1097, acc 0.9565\n",
      "Epoch 98: Train loss 0.1050, acc 0.9623 | Val loss 0.1085, acc 0.9592\n",
      "Epoch 99: Train loss 0.1090, acc 0.9595 | Val loss 0.1118, acc 0.9565\n",
      "Epoch 100: Train loss 0.1090, acc 0.9554 | Val loss 0.1118, acc 0.9551\n",
      "Epoch 101: Train loss 0.1109, acc 0.9551 | Val loss 0.1117, acc 0.9585\n",
      "Epoch 102: Train loss 0.1092, acc 0.9573 | Val loss 0.1086, acc 0.9572\n",
      "Epoch 103: Train loss 0.1072, acc 0.9580 | Val loss 0.1145, acc 0.9538\n",
      "Epoch 104: Train loss 0.1073, acc 0.9583 | Val loss 0.1146, acc 0.9545\n",
      "Epoch 105: Train loss 0.1075, acc 0.9583 | Val loss 0.1081, acc 0.9579\n",
      "Epoch 106: Train loss 0.1086, acc 0.9585 | Val loss 0.1116, acc 0.9517\n",
      "Epoch 107: Train loss 0.1068, acc 0.9589 | Val loss 0.1266, acc 0.9504\n",
      "Epoch 108: Train loss 0.1093, acc 0.9592 | Val loss 0.1063, acc 0.9579\n",
      "Epoch 109: Train loss 0.1035, acc 0.9594 | Val loss 0.1085, acc 0.9572\n",
      "Epoch 110: Train loss 0.1004, acc 0.9621 | Val loss 0.1101, acc 0.9551\n",
      "Epoch 111: Train loss 0.0962, acc 0.9621 | Val loss 0.1071, acc 0.9572\n",
      "Epoch 112: Train loss 0.1002, acc 0.9619 | Val loss 0.1069, acc 0.9545\n",
      "Epoch 113: Train loss 0.0971, acc 0.9623 | Val loss 0.1028, acc 0.9613\n",
      "Epoch 114: Train loss 0.1023, acc 0.9626 | Val loss 0.1166, acc 0.9497\n",
      "Epoch 115: Train loss 0.1050, acc 0.9578 | Val loss 0.1034, acc 0.9592\n",
      "Epoch 116: Train loss 0.0986, acc 0.9640 | Val loss 0.1126, acc 0.9531\n",
      "Epoch 117: Train loss 0.0939, acc 0.9640 | Val loss 0.1112, acc 0.9572\n",
      "Epoch 118: Train loss 0.1054, acc 0.9587 | Val loss 0.1075, acc 0.9592\n",
      "Epoch 119: Train loss 0.0944, acc 0.9619 | Val loss 0.1091, acc 0.9626\n",
      "Epoch 120: Train loss 0.0971, acc 0.9594 | Val loss 0.1245, acc 0.9477\n",
      "Epoch 121: Train loss 0.1060, acc 0.9561 | Val loss 0.1031, acc 0.9606\n",
      "Epoch 122: Train loss 0.1103, acc 0.9572 | Val loss 0.1100, acc 0.9585\n",
      "Epoch 123: Train loss 0.1010, acc 0.9607 | Val loss 0.1126, acc 0.9558\n",
      "Epoch 124: Train loss 0.1012, acc 0.9609 | Val loss 0.1246, acc 0.9504\n",
      "Epoch 125: Train loss 0.1035, acc 0.9585 | Val loss 0.1094, acc 0.9592\n",
      "Epoch 126: Train loss 0.0968, acc 0.9633 | Val loss 0.1068, acc 0.9592\n",
      "Epoch 127: Train loss 0.0932, acc 0.9609 | Val loss 0.1000, acc 0.9626\n",
      "Epoch 128: Train loss 0.0973, acc 0.9623 | Val loss 0.1037, acc 0.9579\n",
      "Epoch 129: Train loss 0.0978, acc 0.9589 | Val loss 0.1052, acc 0.9606\n",
      "Epoch 130: Train loss 0.0910, acc 0.9653 | Val loss 0.1245, acc 0.9497\n",
      "Epoch 131: Train loss 0.0998, acc 0.9606 | Val loss 0.1069, acc 0.9606\n",
      "Epoch 132: Train loss 0.0928, acc 0.9650 | Val loss 0.1046, acc 0.9579\n",
      "Epoch 133: Train loss 0.0953, acc 0.9612 | Val loss 0.1031, acc 0.9613\n",
      "Epoch 134: Train loss 0.0949, acc 0.9628 | Val loss 0.1063, acc 0.9619\n",
      "Epoch 135: Train loss 0.0994, acc 0.9604 | Val loss 0.1033, acc 0.9646\n",
      "Epoch 136: Train loss 0.0916, acc 0.9643 | Val loss 0.1147, acc 0.9538\n",
      "Epoch 137: Train loss 0.0992, acc 0.9617 | Val loss 0.1014, acc 0.9633\n",
      "Epoch 138: Train loss 0.0955, acc 0.9634 | Val loss 0.1022, acc 0.9626\n",
      "Epoch 139: Train loss 0.0965, acc 0.9641 | Val loss 0.1073, acc 0.9585\n",
      "Epoch 140: Train loss 0.0994, acc 0.9602 | Val loss 0.1239, acc 0.9497\n",
      "Epoch 141: Train loss 0.0972, acc 0.9616 | Val loss 0.1044, acc 0.9592\n",
      "Epoch 142: Train loss 0.0926, acc 0.9624 | Val loss 0.1058, acc 0.9599\n",
      "Early stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.71      0.75       496\n",
      "           1       0.77      0.81      0.79       471\n",
      "           2       0.80      0.85      0.83       420\n",
      "           3       0.95      0.82      0.88       491\n",
      "           4       0.87      0.96      0.91       532\n",
      "           5       0.99      1.00      0.99       537\n",
      "\n",
      "    accuracy                           0.86      2947\n",
      "   macro avg       0.86      0.86      0.86      2947\n",
      "weighted avg       0.87      0.86      0.86      2947\n",
      "\n",
      "Artifacts saved to ./artifacts32/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import shutil  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- 32-feature extractor ---\n",
    "def extract_32_features(acc_window: np.ndarray, gyro_window: np.ndarray) -> np.ndarray:\n",
    "    features = []\n",
    "    # Accelerometer stats (x,y,z)\n",
    "    for i in range(3):\n",
    "        data = acc_window[:, i]\n",
    "        features.extend([data.mean(), data.std(), data.min(), data.max()])\n",
    "    # Gyroscope stats (x,y,z)\n",
    "    for i in range(3):\n",
    "        data = gyro_window[:, i]\n",
    "        features.extend([data.mean(), data.std(), data.min(), data.max()])\n",
    "    # Magnitude stats\n",
    "    for mag in (np.linalg.norm(acc_window, axis=1), np.linalg.norm(gyro_window, axis=1)):\n",
    "        features.extend([mag.mean(), mag.std(), mag.min(), mag.max()])\n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "# # 1. Extract dataset\n",
    "# zip_path = \"/content/UCI HAR Dataset.zip\"  # adjust if needed\n",
    "# extract_path = \"/content/\"\n",
    "# with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "#     z.extractall(extract_path)\n",
    "DATASET_PATH = 'UCI HAR Dataset'\n",
    "\n",
    "# 2. Dataset class\n",
    "class HARDataset(Dataset):\n",
    "    def __init__(self, dataset_path, split='train'):\n",
    "        self.dataset_path = dataset_path\n",
    "        # load raw windows\n",
    "        def load_signal(name):\n",
    "            fname = os.path.join(dataset_path, split, 'Inertial Signals', f'{name}_{split}.txt')\n",
    "            return np.loadtxt(fname)\n",
    "        # stack windows\n",
    "        acc = np.stack([\n",
    "            load_signal('total_acc_x'),\n",
    "            load_signal('total_acc_y'),\n",
    "            load_signal('total_acc_z')\n",
    "        ], axis=2)\n",
    "        gyro = np.stack([\n",
    "            load_signal('body_gyro_x'),\n",
    "            load_signal('body_gyro_y'),\n",
    "            load_signal('body_gyro_z')\n",
    "        ], axis=2)\n",
    "        # extract features per window\n",
    "        X = np.array([extract_32_features(acc[i], gyro[i]) for i in range(acc.shape[0])])\n",
    "        y = np.loadtxt(os.path.join(dataset_path, split, f'y_{split}.txt'), dtype=int) - 1  # zero-based\n",
    "        self.X = X\n",
    "        self.y = y.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# 3. Preprocessing\n",
    "train_dataset = HARDataset(DATASET_PATH, 'train')\n",
    "test_dataset = HARDataset(DATASET_PATH, 'test')\n",
    "\n",
    "# Fit scaler and encoder on train\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_dataset.X)\n",
    "test_X = scaler.transform(test_dataset.X)\n",
    "label_enc = LabelEncoder()\n",
    "train_y = label_enc.fit_transform(train_dataset.y)\n",
    "test_y = label_enc.transform(test_dataset.y)\n",
    "\n",
    "# Convert to tensors and wrap back into dataset\n",
    "train_dataset.X = torch.from_numpy(train_X)\n",
    "train_dataset.y = torch.from_numpy(train_y)\n",
    "test_dataset.X = torch.from_numpy(test_X)\n",
    "test_dataset.y = torch.from_numpy(test_y)\n",
    "\n",
    "# Split validation from train\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    train_dataset.X.numpy(), train_dataset.y.numpy(),\n",
    "    test_size=0.2, stratify=train_dataset.y.numpy(), random_state=42\n",
    ")\n",
    "train_dataset.X = torch.from_numpy(X_tr)\n",
    "train_dataset.y = torch.from_numpy(y_tr)\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "# 4. DataLoaders\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 5. Model definition\n",
    "class HARNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(), nn.BatchNorm1d(256), nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.BatchNorm1d(128), nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64), nn.ReLU(), nn.BatchNorm1d(64), nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# initialize\n",
    "input_dim = train_dataset.X.shape[1]\n",
    "num_classes = len(label_enc.classes_)\n",
    "model = HARNet(input_dim, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6. Training loop with validation & checkpoint\n",
    "best_val_loss = float('inf')\n",
    "epochs, patience = 200, 15\n",
    "patience_counter = 0\n",
    "checkpoint_path = 'artifacts32/best_model.pth'\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward(); optimizer.step()\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += X_batch.size(0)\n",
    "    train_loss /= total; train_acc = correct/total\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_correct += (preds == y_batch).sum().item()\n",
    "            val_total += X_batch.size(0)\n",
    "    val_loss /= val_total; val_acc = val_correct/val_total\n",
    "    print(f\"Epoch {epoch}: Train loss {train_loss:.4f}, acc {train_acc:.4f} | Val loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# 7. Evaluation on test set\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        logits = model(X_batch)\n",
    "        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        all_targets.extend(y_batch.numpy())\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(all_targets, all_preds, target_names=label_enc.classes_.astype(str)))\n",
    "\n",
    "# 8. Save artifacts\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "torch.save(model.state_dict(), 'artifacts32/best_model.pth')\n",
    "joblib.dump(scaler, 'artifacts32/scaler.joblib')\n",
    "joblib.dump(label_enc, 'artifacts32/label_encoder.joblib')\n",
    "shutil.copy(\n",
    "    os.path.join(DATASET_PATH, 'activity_labels.txt'),\n",
    "    'artifacts32/activity_labels.txt'\n",
    ")\n",
    "\n",
    "print(\"Artifacts saved to ./artifacts32/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train 0.9838/0.6419 | Val 0.9893/0.7648\n",
      "Epoch 2: Train 0.5163/0.8240 | Val 0.4162/0.8532\n",
      "Epoch 3: Train 0.3868/0.8653 | Val 0.3000/0.8702\n",
      "Epoch 4: Train 0.3236/0.8832 | Val 0.2553/0.8906\n",
      "Epoch 5: Train 0.2844/0.8954 | Val 0.2434/0.8994\n",
      "Epoch 6: Train 0.2684/0.9015 | Val 0.2145/0.9157\n",
      "Epoch 7: Train 0.2461/0.9089 | Val 0.2074/0.9177\n",
      "Epoch 8: Train 0.2445/0.9124 | Val 0.2097/0.9225\n",
      "Epoch 9: Train 0.2250/0.9167 | Val 0.2086/0.9232\n",
      "Epoch 10: Train 0.2105/0.9235 | Val 0.1870/0.9273\n",
      "Epoch 11: Train 0.2086/0.9189 | Val 0.1803/0.9320\n",
      "Epoch 12: Train 0.2047/0.9247 | Val 0.1930/0.9252\n",
      "Epoch 13: Train 0.1966/0.9259 | Val 0.1815/0.9252\n",
      "Epoch 14: Train 0.1941/0.9259 | Val 0.1745/0.9327\n",
      "Epoch 15: Train 0.1828/0.9325 | Val 0.1668/0.9361\n",
      "Epoch 16: Train 0.1723/0.9342 | Val 0.1624/0.9341\n",
      "Epoch 17: Train 0.1711/0.9345 | Val 0.1657/0.9354\n",
      "Epoch 18: Train 0.1803/0.9305 | Val 0.1518/0.9388\n",
      "Epoch 19: Train 0.1628/0.9366 | Val 0.1640/0.9341\n",
      "Epoch 20: Train 0.1697/0.9339 | Val 0.1464/0.9395\n",
      "Epoch 21: Train 0.1604/0.9379 | Val 0.1474/0.9402\n",
      "Epoch 22: Train 0.1618/0.9376 | Val 0.1499/0.9395\n",
      "Epoch 23: Train 0.1573/0.9393 | Val 0.1375/0.9463\n",
      "Epoch 24: Train 0.1482/0.9447 | Val 0.1348/0.9470\n",
      "Epoch 25: Train 0.1572/0.9395 | Val 0.1455/0.9449\n",
      "Epoch 26: Train 0.1530/0.9418 | Val 0.1374/0.9497\n",
      "Epoch 27: Train 0.1460/0.9451 | Val 0.1333/0.9470\n",
      "Epoch 28: Train 0.1424/0.9471 | Val 0.1358/0.9477\n",
      "Epoch 29: Train 0.1429/0.9432 | Val 0.1504/0.9375\n",
      "Epoch 30: Train 0.1472/0.9395 | Val 0.1544/0.9429\n",
      "Epoch 31: Train 0.1435/0.9468 | Val 0.1674/0.9313\n",
      "Epoch 32: Train 0.1455/0.9452 | Val 0.1341/0.9443\n",
      "Epoch 33: Train 0.1308/0.9493 | Val 0.1308/0.9449\n",
      "Epoch 34: Train 0.1271/0.9515 | Val 0.1366/0.9463\n",
      "Epoch 35: Train 0.1412/0.9463 | Val 0.1435/0.9497\n",
      "Epoch 36: Train 0.1370/0.9480 | Val 0.1295/0.9483\n",
      "Epoch 37: Train 0.1324/0.9468 | Val 0.1258/0.9504\n",
      "Epoch 38: Train 0.1238/0.9522 | Val 0.1450/0.9436\n",
      "Epoch 39: Train 0.1261/0.9510 | Val 0.1588/0.9368\n",
      "Epoch 40: Train 0.1239/0.9515 | Val 0.1317/0.9483\n",
      "Epoch 41: Train 0.1197/0.9536 | Val 0.1254/0.9524\n",
      "Epoch 42: Train 0.1242/0.9498 | Val 0.1245/0.9538\n",
      "Epoch 43: Train 0.1128/0.9578 | Val 0.1207/0.9538\n",
      "Epoch 44: Train 0.1142/0.9594 | Val 0.1222/0.9531\n",
      "Epoch 45: Train 0.1078/0.9570 | Val 0.1199/0.9545\n",
      "Epoch 46: Train 0.1205/0.9515 | Val 0.1240/0.9538\n",
      "Epoch 47: Train 0.1086/0.9578 | Val 0.1262/0.9504\n",
      "Epoch 48: Train 0.1062/0.9570 | Val 0.1127/0.9572\n",
      "Epoch 49: Train 0.1110/0.9568 | Val 0.1190/0.9565\n",
      "Epoch 50: Train 0.1131/0.9578 | Val 0.1347/0.9483\n",
      "Epoch 51: Train 0.1116/0.9558 | Val 0.1289/0.9531\n",
      "Epoch 52: Train 0.1195/0.9524 | Val 0.1197/0.9558\n",
      "Epoch 53: Train 0.1118/0.9536 | Val 0.1203/0.9558\n",
      "Epoch 54: Train 0.1131/0.9561 | Val 0.1239/0.9517\n",
      "Epoch 55: Train 0.1102/0.9565 | Val 0.1245/0.9531\n",
      "Epoch 56: Train 0.1051/0.9572 | Val 0.1113/0.9585\n",
      "Epoch 57: Train 0.1020/0.9628 | Val 0.1126/0.9579\n",
      "Epoch 58: Train 0.0958/0.9614 | Val 0.1045/0.9585\n",
      "Epoch 59: Train 0.1040/0.9573 | Val 0.1075/0.9579\n",
      "Epoch 60: Train 0.0946/0.9624 | Val 0.1052/0.9606\n",
      "Epoch 61: Train 0.1026/0.9609 | Val 0.1044/0.9592\n",
      "Epoch 62: Train 0.0922/0.9655 | Val 0.1050/0.9592\n",
      "Epoch 63: Train 0.0977/0.9602 | Val 0.1215/0.9524\n",
      "Epoch 64: Train 0.0979/0.9623 | Val 0.1043/0.9626\n",
      "Epoch 65: Train 0.0900/0.9629 | Val 0.1040/0.9613\n",
      "Epoch 66: Train 0.0888/0.9641 | Val 0.1050/0.9626\n",
      "Epoch 67: Train 0.0833/0.9655 | Val 0.1119/0.9579\n",
      "Epoch 68: Train 0.0894/0.9629 | Val 0.1027/0.9626\n",
      "Epoch 69: Train 0.0897/0.9645 | Val 0.1134/0.9558\n",
      "Epoch 70: Train 0.0914/0.9653 | Val 0.1031/0.9613\n",
      "Epoch 71: Train 0.0807/0.9691 | Val 0.1062/0.9626\n",
      "Epoch 72: Train 0.0871/0.9650 | Val 0.1192/0.9558\n",
      "Epoch 73: Train 0.0832/0.9675 | Val 0.1040/0.9579\n",
      "Epoch 74: Train 0.0811/0.9680 | Val 0.1143/0.9558\n",
      "Epoch 75: Train 0.0850/0.9677 | Val 0.1060/0.9613\n",
      "Epoch 76: Train 0.0806/0.9674 | Val 0.1075/0.9599\n",
      "Epoch 77: Train 0.0779/0.9679 | Val 0.1026/0.9606\n",
      "Epoch 78: Train 0.0794/0.9697 | Val 0.0993/0.9633\n",
      "Epoch 79: Train 0.0797/0.9670 | Val 0.1001/0.9640\n",
      "Epoch 80: Train 0.0788/0.9689 | Val 0.1010/0.9619\n",
      "Epoch 81: Train 0.0757/0.9694 | Val 0.0982/0.9667\n",
      "Epoch 82: Train 0.0819/0.9679 | Val 0.0973/0.9653\n",
      "Epoch 83: Train 0.0726/0.9706 | Val 0.0998/0.9640\n",
      "Epoch 84: Train 0.0727/0.9704 | Val 0.0972/0.9653\n",
      "Epoch 85: Train 0.0767/0.9699 | Val 0.0989/0.9660\n",
      "Epoch 86: Train 0.0782/0.9672 | Val 0.0974/0.9633\n",
      "Epoch 87: Train 0.0784/0.9691 | Val 0.0974/0.9640\n",
      "Epoch 88: Train 0.0785/0.9699 | Val 0.0952/0.9653\n",
      "Epoch 89: Train 0.0772/0.9679 | Val 0.1076/0.9579\n",
      "Epoch 90: Train 0.0766/0.9697 | Val 0.0964/0.9660\n",
      "Epoch 91: Train 0.0762/0.9697 | Val 0.1010/0.9626\n",
      "Epoch 92: Train 0.0733/0.9719 | Val 0.1034/0.9619\n",
      "Epoch 93: Train 0.0723/0.9694 | Val 0.1013/0.9646\n",
      "Epoch 94: Train 0.0751/0.9701 | Val 0.0927/0.9674\n",
      "Epoch 95: Train 0.0733/0.9708 | Val 0.0936/0.9680\n",
      "Epoch 96: Train 0.0727/0.9711 | Val 0.0991/0.9653\n",
      "Epoch 97: Train 0.0817/0.9650 | Val 0.1030/0.9633\n",
      "Epoch 98: Train 0.0717/0.9696 | Val 0.1001/0.9640\n",
      "Epoch 99: Train 0.0743/0.9692 | Val 0.0931/0.9687\n",
      "Epoch 100: Train 0.0732/0.9684 | Val 0.0991/0.9653\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       496\n",
      "           1       0.85      0.80      0.83       471\n",
      "           2       0.82      0.88      0.85       420\n",
      "           3       0.95      0.80      0.87       491\n",
      "           4       0.85      0.96      0.90       532\n",
      "           5       0.99      1.00      1.00       537\n",
      "\n",
      "    accuracy                           0.88      2947\n",
      "   macro avg       0.88      0.88      0.88      2947\n",
      "weighted avg       0.88      0.88      0.88      2947\n",
      "\n",
      "Artifacts saved to /artifacts/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- 32-feature extractor with optional jerk features ---\n",
    "def extract_32_features(acc_window: np.ndarray, gyro_window: np.ndarray) -> np.ndarray:\n",
    "    features = []\n",
    "    # Accelerometer stats (x,y,z)\n",
    "    for data in [acc_window[:, i] for i in range(3)]:\n",
    "        features.extend([data.mean(), data.std(), data.min(), data.max()])\n",
    "    # Gyroscope stats (x,y,z)\n",
    "    for data in [gyro_window[:, i] for i in range(3)]:\n",
    "        features.extend([data.mean(), data.std(), data.min(), data.max()])\n",
    "    # Magnitude stats\n",
    "    for mag in (np.linalg.norm(acc_window, axis=1), np.linalg.norm(gyro_window, axis=1)):\n",
    "        features.extend([mag.mean(), mag.std(), mag.min(), mag.max()])\n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "# # 1. Extract dataset\n",
    "# zip_path = \"/content/UCI HAR Dataset.zip\"  # adjust if needed\n",
    "# extract_path = \"/content/\"\n",
    "# with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "#     z.extractall(extract_path)\n",
    "DATASET_PATH = 'UCI HAR Dataset'\n",
    "\n",
    "# 2. Dataset class\n",
    "class HARDataset(Dataset):\n",
    "    def __init__(self, dataset_path, split='train'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.X, self.y = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        # load raw signals\n",
    "        def load_signal(name):\n",
    "            fname = os.path.join(self.dataset_path, self.split, 'Inertial Signals', f'{name}_{self.split}.txt')\n",
    "            return np.loadtxt(fname)\n",
    "        acc = np.stack([load_signal(f'total_acc_{axis}') for axis in ('x','y','z')], axis=2)\n",
    "        gyro = np.stack([load_signal(f'body_gyro_{axis}') for axis in ('x','y','z')], axis=2)\n",
    "        # feature extraction\n",
    "        X = np.array([extract_32_features(acc[i], gyro[i]) for i in range(acc.shape[0])])\n",
    "        y = np.loadtxt(os.path.join(self.dataset_path, self.split, f'y_{self.split}.txt'), dtype=int) - 1\n",
    "        return X, y.astype(np.int64)\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "# 3. Prepare datasets and scalers\n",
    "train_ds = HARDataset(DATASET_PATH, 'train')\n",
    "test_ds = HARDataset(DATASET_PATH, 'test')\n",
    "\n",
    "scaler = StandardScaler().fit(train_ds.X)\n",
    "train_X = scaler.transform(train_ds.X)\n",
    "test_X = scaler.transform(test_ds.X)\n",
    "label_enc = LabelEncoder().fit(train_ds.y)\n",
    "train_y = label_enc.transform(train_ds.y)\n",
    "test_y = label_enc.transform(test_ds.y)\n",
    "\n",
    "# Convert to tensors\n",
    "train_ds.X, train_ds.y = torch.from_numpy(train_X), torch.from_numpy(train_y)\n",
    "test_ds.X, test_ds.y = torch.from_numpy(test_X), torch.from_numpy(test_y)\n",
    "\n",
    "# Split validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    train_ds.X.numpy(), train_ds.y.numpy(),\n",
    "    test_size=0.2, stratify=train_ds.y.numpy(), random_state=42\n",
    ")\n",
    "train_ds.X, train_ds.y = torch.from_numpy(X_tr), torch.from_numpy(y_tr)\n",
    "val_ds = torch.utils.data.TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 512  # reduced for more gradient updates\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 4. Improved Model with additional layer and LeakyReLU\n",
    "class HARNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.LeakyReLU(), nn.BatchNorm1d(512), nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256), nn.LeakyReLU(), nn.BatchNorm1d(256), nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128), nn.LeakyReLU(), nn.BatchNorm1d(128), nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64), nn.LeakyReLU(), nn.BatchNorm1d(64), nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# Initialize\n",
    "model = HARNet(train_ds.X.shape[1], len(label_enc.classes_)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "# 5. Training loop\n",
    "best_val_loss = float('inf')\n",
    "epochs, patience = 100, 10\n",
    "patience_counter = 0\n",
    "checkpoint = 'artifacts/best_model.pth'\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(Xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "        preds = out.argmax(dim=1)\n",
    "        total_correct += (preds == yb).sum().item()\n",
    "        total_samples += Xb.size(0)\n",
    "    train_loss = total_loss/total_samples\n",
    "    train_acc = total_correct/total_samples\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_samples = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in val_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            out = model(Xb)\n",
    "            loss = criterion(out, yb)\n",
    "            val_loss += loss.item() * Xb.size(0)\n",
    "            preds = out.argmax(dim=1)\n",
    "            val_correct += (preds == yb).sum().item()\n",
    "            val_samples += Xb.size(0)\n",
    "    val_loss /= val_samples\n",
    "    val_acc = val_correct/val_samples\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch}: Train {train_loss:.4f}/{train_acc:.4f} | Val {val_loss:.4f}/{val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss, patience_counter = val_loss, 0\n",
    "        torch.save(model.state_dict(), checkpoint)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# 6. Test evaluation\n",
    "model.load_state_dict(torch.load(checkpoint))\n",
    "model.eval()\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        Xb = Xb.to(device)\n",
    "        out = model(Xb)\n",
    "        all_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
    "        all_targets.extend(yb.numpy())\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(all_targets, all_preds, target_names=label_enc.classes_.astype(str)))\n",
    "\n",
    "# 7. Save artifacts\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "torch.save(model.state_dict(), checkpoint)\n",
    "joblib.dump(scaler, 'artifacts/scaler.joblib')\n",
    "joblib.dump(label_enc, 'artifacts/label_encoder.joblib')\n",
    "shutil.copy(\n",
    "    os.path.join(DATASET_PATH, 'activity_labels.txt'),\n",
    "    'artifacts/activity_labels.txt'\n",
    ")\n",
    "print(\"Artifacts saved to /artifacts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
