# -*- coding: utf-8 -*-
"""roberta_balancedWHATSAP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GArvtTVYQeA8MIZxmBTeK6r-fqSCPLDq
"""

!pip install -q datasets
!pip install -q transformers[torch]
!pip install -q accelerate -U
!pip install -q imblearn

# Import necessary libraries
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
from datasets import Dataset, DatasetDict
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Check if CUDA is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Step 1: Load the dataset
dataset_path = 'balanced_combined_and_shuffled.xlsx'
data = pd.read_excel(dataset_path)

# Step 2: Convert text data into numerical form using TF-IDF
vectorizer = TfidfVectorizer(max_features=10000)
X = vectorizer.fit_transform(data['CombinedTweet']).toarray()
y = data['Tag']

# Step 3: Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

# Step 4: Create a new DataFrame based on SMOTE-resampled data
balanced_data = pd.DataFrame(X_res, columns=vectorizer.get_feature_names_out())
balanced_data['CombinedTweet'] = vectorizer.inverse_transform(X_res)
balanced_data['CombinedTweet'] = balanced_data['CombinedTweet'].apply(lambda x: ' '.join(x))
balanced_data['Tag'] = y_res

# Verify the new balanced data has an equal number of classes
print("Balanced data label distribution:")
print(balanced_data['Tag'].value_counts())

# Step 5: Split the balanced data
train_texts, test_texts, train_labels, test_labels = train_test_split(
    balanced_data['CombinedTweet'].tolist(), balanced_data['Tag'].tolist(), test_size=0.2, random_state=42)

# Step 6: Tokenize the text
tokenizer = AutoTokenizer.from_pretrained("urduhack/roberta-urdu-small")
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

# Step 7: Create Dataset objects
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],
    'attention_mask': train_encodings['attention_mask'],
    'labels': train_labels  # Ensure labels are in list format
})

test_dataset = Dataset.from_dict({
    'input_ids': test_encodings['input_ids'],
    'attention_mask': test_encodings['attention_mask'],
    'labels': test_labels  # Ensure labels are in list format
})

# Combine into a DatasetDict
dataset = DatasetDict({
    'train': train_dataset,
    'test': test_dataset
})

# Step 8: Define the model
model = AutoModelForSequenceClassification.from_pretrained("urduhack/roberta-urdu-small", num_labels=2)
model.to(device)
!pip install -q wandb
import os
os.environ["WANDB_DISABLED"] = "true"

# Step 9: Define the compute metrics function
def compute_metrics(p):
    preds = p.predictions.argmax(-1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# Step 10: Set training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=15,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    report_to="none"
)

# Step 11: Create the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test'],
    compute_metrics=compute_metrics
)

# Step 12: Train the model
trainer.train()

# Step 13: Evaluate the model
eval_results = trainer.evaluate()

print("Evaluation results:")
for key, value in eval_results.items():
    print(f"{key}: {value}")

# Function to get random samples
def get_random_samples(data, n=5):
    random_samples = data.sample(n)
    return random_samples['CombinedTweet'].tolist()
# Function to predict on unseen examples
def predict_unseen_examples(unseen_texts, model, tokenizer, device):
    model.eval()  # Set model to evaluation mode
    inputs = tokenizer(unseen_texts, return_tensors="pt", truncation=True, padding=True)
    inputs = {key: value.to(device) for key, value in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=-1)

    return predictions.cpu().numpy()

# Get random samples
unseen_texts = get_random_samples(data)

# Get predictions for the unseen examples
unseen_predictions = predict_unseen_examples(unseen_texts, model, tokenizer, device)

# Print the results
for text, prediction in zip(unseen_texts, unseen_predictions):
    print(f"Text: {text}\nPrediction: {prediction}\n")

# prompt: mount drive and save this model to drive

from google.colab import drive
drive.mount('/content/drive')

# Save the model to Google Drive
model_path = "/content/drive/MyDrive/Roberta-model.pt"
torch.save(model.state_dict(), model_path)